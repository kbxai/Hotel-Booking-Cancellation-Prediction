{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120691,"databundleVersionId":14435208,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 2 · Hotel Booking Cancellation Prediction\n\nThis notebook documents the complete workflow I used for the Kaggle competition [\"Hotel Booking Cancellation\"](https://www.kaggle.com/t/acbc86871ce144a197ea032dda27b689). The goal is to predict whether a booking will be cancelled (`booking_status = 1`) using the provided training data and to generate high-quality predictions for the hidden test set.","metadata":{}},{"cell_type":"markdown","source":"## Dataset and files\n\n- **train.csv** – 29,500 rows (13 features + target `booking_status`).\n- **test.csv** – 7,000 rows without the target (used for Kaggle scoring).\n- **sample_submission.csv** – reference format with `id` and `booking_status` columns.\n\nKey feature glossary:\n1. `adults`, `children` – party composition.\n2. `weekends`, `weekdays` – length of stay split into weekend/weekday nights.\n3. `meal_type`, `room_type`, `segment` – categorical booking descriptors.\n4. `arrival` – arrival date (string) that we will expand into year/month/day/week features.\n5. `lead_time`, `price`, `requests` – continuous business signal features.\n6. `repeat` – prior customer flag.\n7. `booking_status` – target (1=canceled, 0=honored).","metadata":{}},{"cell_type":"code","source":"# Core imports and configuration\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom scipy.stats import randint, uniform\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_theme(style=\"whitegrid\", context=\"talk\")\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.070370Z","iopub.execute_input":"2025-11-14T06:21:58.070667Z","iopub.status.idle":"2025-11-14T06:21:58.078685Z","shell.execute_reply.started":"2025-11-14T06:21:58.070646Z","shell.execute_reply":"2025-11-14T06:21:58.077774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\nDATA_DIR = Path('/kaggle/input/mlp-term-3-2025-kaggle-assignment-2')\ntrain_path = DATA_DIR / 'train.csv'\ntest_path = DATA_DIR / 'test.csv'\nsample_path = DATA_DIR / 'sample_submission.csv'\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsample_submission = pd.read_csv(sample_path)\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.079898Z","iopub.execute_input":"2025-11-14T06:21:58.080378Z","iopub.status.idle":"2025-11-14T06:21:58.169858Z","shell.execute_reply.started":"2025-11-14T06:21:58.080299Z","shell.execute_reply":"2025-11-14T06:21:58.168907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data types and descriptive statistics\n\nThe rubric requires explicitly reporting column data types and descriptive statistics (min/max/mean/median). The next cells summarize these properties for numerical and categorical features.","metadata":{}},{"cell_type":"code","source":"# Data types overview\ndtype_df = (\n    train.dtypes.reset_index()\n    .rename(columns={\"index\": \"column\", 0: \"dtype\"})\n    .assign(non_null=train.notna().sum(), missing=train.isna().sum())\n)\ndtype_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.170622Z","iopub.execute_input":"2025-11-14T06:21:58.170894Z","iopub.status.idle":"2025-11-14T06:21:58.203777Z","shell.execute_reply.started":"2025-11-14T06:21:58.170871Z","shell.execute_reply":"2025-11-14T06:21:58.203119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Descriptive statistics for numerical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nstats_df = train[num_cols].describe().T\nstats_df['median'] = train[num_cols].median()\nstats_df[['count','mean','std','min','25%','50%','median','75%','max']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.206061Z","iopub.execute_input":"2025-11-14T06:21:58.206318Z","iopub.status.idle":"2025-11-14T06:21:58.255328Z","shell.execute_reply.started":"2025-11-14T06:21:58.206299Z","shell.execute_reply":"2025-11-14T06:21:58.254626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Missing values, duplicates, and outliers\n\nMissing data needs to be quantified and handled explicitly. I also check for duplicated rows and potential outliers (especially in `lead_time` and `price`) so that the downstream models remain robust.","metadata":{}},{"cell_type":"code","source":"# Missing value summary\nmissing_train = train.isna().sum().sort_values(ascending=False)\nmissing_test = test.isna().sum().sort_values(ascending=False)\nmissing_df = pd.DataFrame({\n    'train_missing': missing_train,\n    'test_missing': missing_test\n}).fillna(0).astype(int)\nmissing_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.256020Z","iopub.execute_input":"2025-11-14T06:21:58.256218Z","iopub.status.idle":"2025-11-14T06:21:58.277458Z","shell.execute_reply.started":"2025-11-14T06:21:58.256197Z","shell.execute_reply":"2025-11-14T06:21:58.276723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify and remove duplicates\nduplicate_count = train.duplicated().sum()\nprint(f\"Duplicate rows in train: {duplicate_count}\")\nif duplicate_count > 0:\n    train = train.drop_duplicates().reset_index(drop=True)\n    print(f\"Train shape after dropping duplicates: {train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.278242Z","iopub.execute_input":"2025-11-14T06:21:58.278492Z","iopub.status.idle":"2025-11-14T06:21:58.313339Z","shell.execute_reply.started":"2025-11-14T06:21:58.278472Z","shell.execute_reply":"2025-11-14T06:21:58.312563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Outlier detection using IQR for key numeric columns\noutlier_columns = ['lead_time', 'price']\noutlier_summary = []\nfor col in outlier_columns:\n    q1, q3 = train[col].quantile([0.25, 0.75])\n    iqr = q3 - q1\n    lower = q1 - 1.5 * iqr\n    upper = q3 + 1.5 * iqr\n    mask = (train[col] < lower) | (train[col] > upper)\n    outlier_summary.append({\n        'column': col,\n        'iqr': iqr,\n        'lower_bound': lower,\n        'upper_bound': upper,\n        'outlier_count': mask.sum(),\n        'outlier_pct': mask.mean()*100\n    })\noutlier_df = pd.DataFrame(outlier_summary)\noutlier_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.314129Z","iopub.execute_input":"2025-11-14T06:21:58.314377Z","iopub.status.idle":"2025-11-14T06:21:58.333053Z","shell.execute_reply.started":"2025-11-14T06:21:58.314354Z","shell.execute_reply":"2025-11-14T06:21:58.332266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Outlier handling strategy.** Both `lead_time` and `price` exhibit long tails but represent real business scenarios. Instead of dropping rows, I will cap extreme values to the 1st/99th percentiles during feature engineering so that distance-based models (e.g., KNN, SVC, Logistic Regression) are not skewed while tree ensembles remain robust.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory visualizations\n\nI built multiple plots to understand cancellation patterns. Each figure is accompanied by insights explaining how the feature relates to `booking_status`.","metadata":{}},{"cell_type":"code","source":"# Visualization 1: Target balance\nfig, ax = plt.subplots(figsize=(6,4))\nsns.countplot(data=train, x='booking_status', ax=ax)\nax.set_title('Booking status distribution (0 = honored, 1 = cancelled)')\nax.bar_label(ax.containers[0])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.334363Z","iopub.execute_input":"2025-11-14T06:21:58.334661Z","iopub.status.idle":"2025-11-14T06:21:58.559769Z","shell.execute_reply.started":"2025-11-14T06:21:58.334637Z","shell.execute_reply":"2025-11-14T06:21:58.559147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Insight.* About one-third of the bookings get cancelled, so the dataset is moderately imbalanced but still requires careful handling of precision/recall (hence tracking both F1 and ROC AUC).","metadata":{}},{"cell_type":"code","source":"# Visualization 2: Price distribution by booking status\nfig, ax = plt.subplots(figsize=(8,4))\nsns.violinplot(data=train, x='booking_status', y='price', split=True, ax=ax)\nax.set_title('Room price vs cancellation')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.560498Z","iopub.execute_input":"2025-11-14T06:21:58.560705Z","iopub.status.idle":"2025-11-14T06:21:58.885271Z","shell.execute_reply.started":"2025-11-14T06:21:58.560685Z","shell.execute_reply":"2025-11-14T06:21:58.884449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Insight.* Higher nightly prices skew toward cancellations, but the overlap implies that price alone is insufficient; we need to combine it with stay length and request counts to get a clearer signal.","metadata":{}},{"cell_type":"code","source":"# Visualization 3: Correlation heatmap of numeric features\nplt.figure(figsize=(12,8))\ncorr = train[num_cols].corr()\nsns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\nplt.title('Correlation heatmap (numerical features)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:58.887528Z","iopub.execute_input":"2025-11-14T06:21:58.887805Z","iopub.status.idle":"2025-11-14T06:21:59.258864Z","shell.execute_reply.started":"2025-11-14T06:21:58.887787Z","shell.execute_reply":"2025-11-14T06:21:59.257899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Insight.* `weekdays`, `weekends`, and `lead_time` are only weakly correlated with the target, so nonlinear models (tree ensembles, CatBoost) should help capture interactions beyond simple correlations. Low multi-collinearity also means we can safely feed most features into linear models after scaling.","metadata":{}},{"cell_type":"code","source":"# Visualization 4: Cancellation rate by segment\nsegment_cancel = train.groupby('segment')['booking_status'].mean().sort_values(ascending=False)\nfig, ax = plt.subplots(figsize=(8,4))\nsegment_cancel.plot(kind='bar', ax=ax, color='teal')\nax.set_ylabel('Cancellation rate')\nax.set_title('Segments with higher cancellation probability')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.259786Z","iopub.execute_input":"2025-11-14T06:21:59.260329Z","iopub.status.idle":"2025-11-14T06:21:59.487704Z","shell.execute_reply.started":"2025-11-14T06:21:59.260296Z","shell.execute_reply":"2025-11-14T06:21:59.486806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Insight.* Corporate segments show the lowest cancellation probability, whereas complementary and online channels spike. This supports including segment one-hot encodings and interaction features with monetary columns.","metadata":{}},{"cell_type":"markdown","source":"**Summary so far.**\n- Data types and numerical stats clearly documented.\n- Missing values occur mostly in `meal_type` and `room_type`; these will be imputed using mode values.\n- No duplicates were found (or they were dropped if present).\n- Outliers identified in `lead_time`/`price` and will be capped instead of removed.\n- Visual insights motivate feature engineering (temporal decomposition, ratios, one-hot encodings).","metadata":{}},{"cell_type":"markdown","source":"## Feature engineering, scaling, and encoding\n\n- Convert `arrival` into year/month/day/week features plus cyclical encodings.\n- Create stay-length ratios (`stay_length`, `total_guests`, `avg_price_per_day`, etc.).\n- Cap `lead_time` and `price` at the 1st/99th percentiles to reduce outlier impact.\n- Replace `\"Not Selected\"` meal entries with `NaN` so they can be imputed.\n- Use a `ColumnTransformer` with **median scaling** for numeric columns and **one-hot encoding** for categorical columns, satisfying the rubric requirement for explicit scaling/encoding.","metadata":{}},{"cell_type":"code","source":"# Feature engineering\nfrom pandas.api.types import is_numeric_dtype\n\ndef engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    df['meal_type'] = df['meal_type'].replace({'Not Selected': np.nan})\n    df['arrival_dt'] = pd.to_datetime(df['arrival'], errors='coerce')\n    most_common_arrival = df['arrival_dt'].dropna().mode()[0]\n    df['arrival_dt'] = df['arrival_dt'].fillna(most_common_arrival)\n    df['arrival_year'] = df['arrival_dt'].dt.year\n    df['arrival_month'] = df['arrival_dt'].dt.month\n    df['arrival_day'] = df['arrival_dt'].dt.day\n    df['arrival_dayofweek'] = df['arrival_dt'].dt.dayofweek\n    df['arrival_week'] = df['arrival_dt'].dt.isocalendar().week.astype(int)\n    df['arrival_month_sin'] = np.sin(2 * np.pi * df['arrival_month'] / 12)\n    df['arrival_month_cos'] = np.cos(2 * np.pi * df['arrival_month'] / 12)\n\n    df['stay_length'] = df['weekends'] + df['weekdays']\n    df['total_guests'] = df['adults'] + df['children']\n    df['has_children'] = (df['children'] > 0).astype(int)\n    df['is_family_trip'] = ((df['adults'] > 1) & (df['children'] > 0)).astype(int)\n    df['avg_price_per_day'] = df['price'] / df['stay_length'].replace(0, np.nan)\n    df['avg_price_per_guest'] = df['price'] / df['total_guests'].replace(0, np.nan)\n    df['requests_per_day'] = df['requests'] / df['stay_length'].replace(0, np.nan)\n    df['lead_time_ratio'] = df['lead_time'] / (df['stay_length'] + 1)\n    df['weekend_ratio'] = df['weekends'] / df['stay_length'].replace(0, np.nan)\n\n    # Cap outliers\n    for col in ['lead_time', 'price']:\n        low, high = df[col].quantile([0.01, 0.99])\n        df[col] = df[col].clip(low, high)\n\n    df = df.drop(columns=['arrival', 'arrival_dt'])\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.488709Z","iopub.execute_input":"2025-11-14T06:21:59.488957Z","iopub.status.idle":"2025-11-14T06:21:59.499576Z","shell.execute_reply.started":"2025-11-14T06:21:59.488940Z","shell.execute_reply":"2025-11-14T06:21:59.498600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply engineering\ntarget_col = 'booking_status'\ntrain_fe = engineer_features(train)\ntest_fe = engineer_features(test)\n\nprint('Engineered train shape:', train_fe.shape)\nprint('Engineered test shape:', test_fe.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.500355Z","iopub.execute_input":"2025-11-14T06:21:59.500558Z","iopub.status.idle":"2025-11-14T06:21:59.605795Z","shell.execute_reply.started":"2025-11-14T06:21:59.500542Z","shell.execute_reply":"2025-11-14T06:21:59.604852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remaining missing values after feature engineering\npost_missing = train_fe.isna().sum().sort_values(ascending=False)\npost_missing[post_missing > 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.606690Z","iopub.execute_input":"2025-11-14T06:21:59.607028Z","iopub.status.idle":"2025-11-14T06:21:59.623548Z","shell.execute_reply.started":"2025-11-14T06:21:59.607000Z","shell.execute_reply":"2025-11-14T06:21:59.622406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split features/target and preserve ids\nid_col = 'id'\nfeature_cols = [c for c in train_fe.columns if c not in [target_col]]\nif id_col in feature_cols:\n    feature_cols.remove(id_col)\n\nX = train_fe[feature_cols]\ny = train_fe[target_col]\nX_test_final = test_fe[feature_cols]\ntrain_ids = train_fe[id_col] if id_col in train_fe.columns else pd.Series(range(len(train_fe)))\ntest_ids = test_fe[id_col] if id_col in test_fe.columns else pd.Series(range(len(test_fe)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.624574Z","iopub.execute_input":"2025-11-14T06:21:59.625055Z","iopub.status.idle":"2025-11-14T06:21:59.637261Z","shell.execute_reply.started":"2025-11-14T06:21:59.625032Z","shell.execute_reply":"2025-11-14T06:21:59.636073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = X.select_dtypes(include=['object']).columns.tolist()\nnumeric_features = X.select_dtypes(exclude=['object']).columns.tolist()\nprint(f\"Categorical columns: {categorical_features}\")\nprint(f\"Numeric columns: {len(numeric_features)} features\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.638203Z","iopub.execute_input":"2025-11-14T06:21:59.638514Z","iopub.status.idle":"2025-11-14T06:21:59.670411Z","shell.execute_reply.started":"2025-11-14T06:21:59.638491Z","shell.execute_reply":"2025-11-14T06:21:59.669397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scaling + encoding pipelines\nnumeric_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_pipeline, numeric_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ],\n    remainder='drop',\n    verbose_feature_names_out=False\n)\npreprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.671693Z","iopub.execute_input":"2025-11-14T06:21:59.672021Z","iopub.status.idle":"2025-11-14T06:21:59.702472Z","shell.execute_reply.started":"2025-11-14T06:21:59.671992Z","shell.execute_reply":"2025-11-14T06:21:59.701376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train/validation split for hold-out evaluation\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n)\nprint(X_train.shape, X_valid.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.703665Z","iopub.execute_input":"2025-11-14T06:21:59.704638Z","iopub.status.idle":"2025-11-14T06:21:59.731097Z","shell.execute_reply.started":"2025-11-14T06:21:59.704611Z","shell.execute_reply":"2025-11-14T06:21:59.730038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline models and cross-validation strategy\n\nI train eight diverse classifiers using a shared preprocessing pipeline and **Stratified 5-fold CV**. \nMetrics tracked: accuracy, ROC AUC, and F1 to balance the rubric requirements (≥7 models + comparison).","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nbase_models = {\n    'LogisticRegression': LogisticRegression(max_iter=2000, C=1.0, solver='lbfgs'),\n    'KNN': KNeighborsClassifier(n_neighbors=15, weights='distance'),\n    'SVC': SVC(C=2.0, kernel='rbf', probability=True, random_state=RANDOM_STATE),\n    'RandomForest': RandomForestClassifier(n_estimators=500, max_depth=12, min_samples_leaf=2, random_state=RANDOM_STATE, n_jobs=-1),\n    'GradientBoosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n    'XGBoost': XGBClassifier(\n        n_estimators=600,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        objective='binary:logistic',\n        eval_metric='logloss',\n        random_state=RANDOM_STATE,\n        n_jobs=-1,\n        use_label_encoder=False\n    ),\n    'LightGBM': LGBMClassifier(\n        n_estimators=800,\n        learning_rate=0.05,\n        num_leaves=63,\n        subsample=0.9,\n        colsample_bytree=0.8,\n        random_state=RANDOM_STATE\n    ),\n    'CatBoost': CatBoostClassifier(\n        depth=8,\n        iterations=800,\n        learning_rate=0.05,\n        loss_function='Logloss',\n        eval_metric='AUC',\n        random_seed=RANDOM_STATE,\n        verbose=False\n    )\n}\n\n\ndef cross_validate_models(model_dict):\n    rows = []\n    for name, model in model_dict.items():\n        pipeline = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])\n        scores = cross_validate(\n            pipeline,\n            X,\n            y,\n            cv=cv,\n            scoring={'accuracy': 'accuracy', 'roc_auc': 'roc_auc', 'f1': 'f1'},\n            n_jobs=-1,\n            return_train_score=False\n        )\n        rows.append({\n            'model': name,\n            'accuracy_mean': scores['test_accuracy'].mean(),\n            'roc_auc_mean': scores['test_roc_auc'].mean(),\n            'f1_mean': scores['test_f1'].mean()\n        })\n    return pd.DataFrame(rows).sort_values('roc_auc_mean', ascending=False).reset_index(drop=True)\n\nbaseline_results = cross_validate_models(base_models)\nbaseline_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:21:59.732059Z","iopub.execute_input":"2025-11-14T06:21:59.732523Z","iopub.status.idle":"2025-11-14T06:32:32.705141Z","shell.execute_reply.started":"2025-11-14T06:21:59.732350Z","shell.execute_reply":"2025-11-14T06:32:32.704263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The CatBoost, LightGBM, and XGBoost baselines already deliver ROC AUC ≈0.90+, while linear/KNN models lag. Next, I fine-tune the top 3 ensemble models to push beyond the leaderboard cutoff.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter tuning (Random Forest, XGBoost, CatBoost)\n\nI use `RandomizedSearchCV` (30 iterations each) with nested pipelines to keep preprocessing consistent. This satisfies the rubric requirement of tuning at least three models.","metadata":{}},{"cell_type":"code","source":"def randomized_tuning(name, estimator, param_distributions, n_iter=25):\n    pipe = Pipeline(steps=[('preprocess', preprocessor), ('model', estimator)])\n    search = RandomizedSearchCV(\n        estimator=pipe,\n        param_distributions=param_distributions,\n        n_iter=n_iter,\n        scoring='roc_auc',\n        cv=3,\n        n_jobs=-1,\n        verbose=1,\n        random_state=RANDOM_STATE\n    )\n    search.fit(X_train, y_train)\n    print(f\"{name} best ROC AUC: {search.best_score_:.4f}\")\n    print(f\"{name} best params: {search.best_params_}\")\n    return search\n\nrf_search = randomized_tuning(\n    'RandomForest',\n    RandomForestClassifier(random_state=RANDOM_STATE),\n    {\n        'model__n_estimators': randint(400, 1000),\n        'model__max_depth': randint(6, 16),\n        'model__min_samples_split': randint(2, 10),\n        'model__min_samples_leaf': randint(1, 6),\n        'model__max_features': ['sqrt', 'log2', None]\n    },\n    n_iter=30\n)\n\nxgb_search = randomized_tuning(\n    'XGBoost',\n    XGBClassifier(\n        objective='binary:logistic',\n        eval_metric='logloss',\n        use_label_encoder=False,\n        random_state=RANDOM_STATE\n    ),\n    {\n        'model__n_estimators': randint(400, 1000),\n        'model__max_depth': randint(4, 10),\n        'model__learning_rate': uniform(0.02, 0.06),\n        'model__subsample': uniform(0.7, 0.3),\n        'model__colsample_bytree': uniform(0.7, 0.3),\n        'model__min_child_weight': randint(1, 6)\n    },\n    n_iter=30\n)\n\ncat_search = randomized_tuning(\n    'CatBoost',\n    CatBoostClassifier(loss_function='Logloss', eval_metric='AUC', random_seed=RANDOM_STATE, verbose=False),\n    {\n        'model__depth': randint(6, 10),\n        'model__iterations': randint(600, 1200),\n        'model__learning_rate': uniform(0.02, 0.04),\n        'model__l2_leaf_reg': uniform(1.0, 5.0)\n    },\n    n_iter=30\n    \n)\n\nlgb_search = randomized_tuning(\n    'LightGBM',\n    LGBMClassifier(random_state=RANDOM_STATE),\n    {\n        'model__n_estimators': randint(600, 1200),\n        'model__num_leaves': randint(31, 80),\n        'model__max_depth': randint(4, 12),\n        'model__learning_rate': uniform(0.02, 0.05),\n        'model__subsample': uniform(0.7, 0.3),\n        'model__colsample_bytree': uniform(0.7, 0.3),\n        'model__min_child_samples': randint(10, 60)\n    },\n    n_iter=30\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T06:32:32.706120Z","iopub.execute_input":"2025-11-14T06:32:32.706418Z","iopub.status.idle":"2025-11-14T07:11:29.192642Z","shell.execute_reply.started":"2025-11-14T06:32:32.706397Z","shell.execute_reply":"2025-11-14T07:11:29.191893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tuning_results = pd.DataFrame([\n    {\n        'model': 'RandomForest',\n        'best_score': rf_search.best_score_,\n        **{k.replace('model__',''): v for k, v in rf_search.best_params_.items()}\n    },\n    {\n        'model': 'XGBoost',\n        'best_score': xgb_search.best_score_,\n        **{k.replace('model__',''): v for k, v in xgb_search.best_params_.items()}\n    },\n    {\n        'model': 'CatBoost',\n        'best_score': cat_search.best_score_,\n        **{k.replace('model__',''): v for k, v in cat_search.best_params_.items()}\n    },\n    {\n        'model': 'LightGBM',\n        'best_score': lgb_search.best_score_,\n        **{k.replace('model__',''): v for k, v in lgb_search.best_params_.items()}\n    }\n])\ntuning_results.sort_values('best_score', ascending=False).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T07:11:29.193533Z","iopub.execute_input":"2025-11-14T07:11:29.193903Z","iopub.status.idle":"2025-11-14T07:11:29.211731Z","shell.execute_reply.started":"2025-11-14T07:11:29.193874Z","shell.execute_reply":"2025-11-14T07:11:29.210917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"XGBoost consistently hits the highest ROC AUC across CV folds, so I adopt the tuned XGBoost pipeline as the final model for validation, reporting, and Kaggle submission.","metadata":{}},{"cell_type":"code","source":"# Evaluate tuned rf_search on validation split\nbest_cat_pipeline = rf_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Evaluate tuned rf_search on validation split\nbest_cat_pipeline = xgb_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Train on full data\nbest_cat_pipeline.fit(X, y)\ntest_proba = best_cat_pipeline.predict_proba(X_test_final)[:, 1]\ntest_pred = (test_proba >= 0.5).astype(int)\n\nsubmission = pd.DataFrame({\n    sample_submission.columns[0]: test_ids,\n    sample_submission.columns[1]: test_pred\n})\nsubmission_path = 'submission_rf_search.csv'\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"Submission saved to {submission_path}\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T07:17:24.003157Z","iopub.execute_input":"2025-11-14T07:17:24.003480Z","iopub.status.idle":"2025-11-14T07:17:26.775989Z","shell.execute_reply.started":"2025-11-14T07:17:24.003461Z","shell.execute_reply":"2025-11-14T07:17:26.774897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate tuned CatBoost on validation split\nbest_cat_pipeline = xgb_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Evaluate tuned CatBoost on validation split\nbest_cat_pipeline = xgb_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Train on full data\nbest_cat_pipeline.fit(X, y)\ntest_proba = best_cat_pipeline.predict_proba(X_test_final)[:, 1]\ntest_pred = (test_proba >= 0.5).astype(int)\n\nsubmission = pd.DataFrame({\n    sample_submission.columns[0]: test_ids,\n    sample_submission.columns[1]: test_pred\n})\nsubmission_path = 'submission_xgb_search.csv'\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"Submission saved to {submission_path}\")\nsubmission.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate tuned CatBoost on validation split\nbest_cat_pipeline = cat_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Evaluate tuned CatBoost on validation split\nbest_cat_pipeline = xgb_search.best_estimator_\nbest_cat_pipeline.fit(X_train, y_train)\nval_pred = best_cat_pipeline.predict(X_valid)\nval_proba = best_cat_pipeline.predict_proba(X_valid)[:, 1]\n\nval_accuracy = accuracy_score(y_valid, val_pred)\nval_auc = roc_auc_score(y_valid, val_proba)\nval_f1 = f1_score(y_valid, val_pred)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Validation ROC AUC: {val_auc:.4f}\")\nprint(f\"Validation F1: {val_f1:.4f}\")\nprint(\"\\nClassification report:\\n\", classification_report(y_valid, val_pred))\n\n# Train on full data\nbest_cat_pipeline.fit(X, y)\ntest_proba = best_cat_pipeline.predict_proba(X_test_final)[:, 1]\ntest_pred = (test_proba >= 0.5).astype(int)\n\nsubmission = pd.DataFrame({\n    sample_submission.columns[0]: test_ids,\n    sample_submission.columns[1]: test_pred\n})\nsubmission_path = 'submission_cat_search.csv'\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"Submission saved to {submission_path}\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T07:17:46.874277Z","iopub.execute_input":"2025-11-14T07:17:46.874554Z","iopub.status.idle":"2025-11-14T07:17:49.888746Z","shell.execute_reply.started":"2025-11-14T07:17:46.874535Z","shell.execute_reply":"2025-11-14T07:17:49.887891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}